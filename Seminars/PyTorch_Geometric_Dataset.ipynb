{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### We're going to create [PyTorch Geometric](https://pytorch-geometric.readthedocs.io/en/latest/) dataset from sctratch. <br> <br> For this purpose we use <font color='Blue'> Cora </font> dataset: \n",
    "https://github.com/kimiyoung/planetoid/tree/master/data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Our goal is to classify graph nodes into one of 7 classes. <br><br> Each node represents one of 2708 publications. Each publication is formed of words from dictionary (containing totally 1433 words). <br>  So, each node in dataset is described by (0/1 valued) feature-vector that encodes the presence of the corresponding words from dictionary."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#### Data contains several <font color='Green'> pickle </font> files with the following extensions: <font color='Green'> [.x, .y, .tx, .ty, .allx, .ally, .graph] </font>."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#### <font color='Green'> .x </font> - contains features of training nodes (140 nodes)\n",
    "#### <font color='Green'> .y </font>- contains one-hot encoded class labels of training nodes\n",
    "#### <font color='Green'> .tx </font> - contains features of test nodes (last 1000 nodes)\n",
    "#### <font color='Green'> .ty </font> - contains labels of test nodes\n",
    "#### <font color='Green'> .allx </font> - contains features of all non-test nodes\n",
    "#### <font color='Green'> .ally </font> - contains labels of all non-test nodes\n",
    "#### <font color='Green'> .graph </font> - contains dictionary of lists <font color='Grey'> {index: [index_of_neighbor_nodes]} </font>, representing neighbored nodes.\n",
    "#### <font color='Green'> .test.index </font> - contains indices of test nodes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#### To sum up, the full dataset contains len(.tx) + len(.allx) = <font color= Blue> 2708 </font> nodes. <br> We use len(.x) = <font color= Blue> 140 </font> nodes for training, len(.tx) = <font color= Blue> 1000 </font> for testing. For validation we use <font color= Blue> 500 </font> nodes from .allx that follow 140 training nodes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <u>Load and preprocess dataset </u>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "import pickle as pkl\n",
    "\n",
    "import numpy as np\n",
    "import scipy.sparse as sp\n",
    "from itertools import repeat\n",
    "\n",
    "\n",
    "DATA_PATH = r'Cora\\raw'\n",
    "dir = os.getcwd()\n",
    "DATA_PATH = os.path.join(dir, DATA_PATH)\n",
    "\n",
    "dataset_str = 'cora'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# load data files\n",
    "\n",
    "names = ['x', 'y', 'tx', 'ty', 'allx', 'ally', 'graph']\n",
    "objects = []\n",
    "for i in range(len(names)):\n",
    "    with open(os.path.join(DATA_PATH, \"ind.{}.{}\".format(dataset_str, names[i])), 'rb') as f:\n",
    "        if sys.version_info > (3, 0):\n",
    "            objects.append(pkl.load(f, encoding='latin1'))\n",
    "        else:\n",
    "            objects.append(pkl.load(f))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 7 objects\n",
      "Type of objects: <class 'scipy.sparse.csr.csr_matrix'>\n",
      "\n",
      "There are 140 train objects\n",
      "\n",
      "There are 1000 test objects\n",
      "\n",
      "There are 1708 other objects\n",
      "\n",
      ".tx + .allx shape is: 2708,\n",
      "which equals total number of instances\n"
     ]
    }
   ],
   "source": [
    "print('There are ' + str(len(objects)) + ' objects',)\n",
    "print('Type of objects:', type(objects[0]), end='\\n\\n')\n",
    "\n",
    "print('There are ' + str(objects[0].toarray().shape[0]) + ' train objects', end='\\n\\n')\n",
    "\n",
    "print('There are ' + str(objects[2].toarray().shape[0]) + ' test objects', end='\\n\\n')\n",
    "\n",
    "print('There are ' + str(objects[4].toarray().shape[0]) + ' other objects', end='\\n\\n')\n",
    "\n",
    "print('.tx + .allx shape is:', objects[2].toarray().shape[0] + objects[4].toarray().shape[0], end=',\\n')\n",
    "print('which equals total number of instances')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000\n",
      "first 10 test inds:  [2692, 2532, 2050, 1715, 2362, 2609, 2622, 1975, 2081, 1767]\n",
      "last 10 test inds:  [1885, 2305, 2354, 2135, 2601, 1770, 1995, 2504, 1749, 2157]\n"
     ]
    }
   ],
   "source": [
    "# let us check indices of test nodes\n",
    "index_file = os.path.join(DATA_PATH, \"ind.{}.test.index\".format(dataset_str))\n",
    "\n",
    "test_index=[]\n",
    "for line in open(index_file):\n",
    "        test_index.append(int(line.strip()))\n",
    "\n",
    "print(len(test_index))\n",
    "print('first 10 test inds: ', test_index[:10])\n",
    "print('last 10 test inds: ', test_index[-10:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "first 10 test inds (ordered): [1708 1709 1710 1711 1712 1713 1714 1715 1716 1717]\n",
      "last 10 test inds (ordered): [2698 2699 2700 2701 2702 2703 2704 2705 2706 2707]\n"
     ]
    }
   ],
   "source": [
    "# as we can see test indices are not ordered. We shall order them\n",
    "sorted_test_index = np.sort(test_index)\n",
    "print('first 10 test inds (ordered):', sorted_test_index[:10])\n",
    "print('last 10 test inds (ordered):', sorted_test_index[-10:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# unpack items\n",
    "x, y, tx, ty, allx, ally, graph = objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "allx shape: (1708, 1433)\n",
      "tx shape: (1000, 1433)\n",
      "\n",
      "merged features shape: (2708, 1433)\n",
      "\n",
      "merged labels shape: (2708,)\n"
     ]
    }
   ],
   "source": [
    "# merge all features data X in one matrix\n",
    "print('allx shape:', allx.toarray().shape)\n",
    "print('tx shape:', tx.toarray().shape, end='\\n\\n')\n",
    "\n",
    "features = sp.vstack((allx, tx)).toarray()\n",
    "print('merged features shape:', features.shape, end='\\n\\n')\n",
    "\n",
    "# do the same for labels Y\n",
    "labels = np.vstack((ally, ty)).argmax(axis=-1)\n",
    "print('merged labels shape:', labels.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# test part is unordered so we order it using indices defined above\n",
    "features[test_index, :] = features[sorted_test_index, :] #scipy version\n",
    "labels[test_index] = labels[sorted_test_index]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <u>Create PyTorch Geometric Dataset </u>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "edge_index shape (duplicated edges):\n",
      "(2, 10858)\n",
      "edge_index shape:\n",
      "(2, 10556)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[   0,    0,    0,  ..., 2707, 2707, 2707],\n",
       "        [ 633, 1862, 2582,  ...,  598, 1473, 2706]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# convert dicts of lists to edge indices\n",
    "\n",
    "import torch\n",
    "\n",
    "row, col = [], []\n",
    "for key, value in graph.items():\n",
    "    row += repeat(key, len(value))\n",
    "    col += value\n",
    "\n",
    "edge_index = np.vstack([row, col])\n",
    "print('edge_index shape (duplicated edges):', edge_index.shape, sep='\\n')\n",
    "\n",
    "# there are some duplicated edges in the original dataset. Lets delete them\n",
    "edge_index = np.unique(edge_index, axis=-1)\n",
    "print('edge_index shape:', edge_index.shape, sep='\\n')\n",
    "\n",
    "edge_index = torch.LongTensor(edge_index)\n",
    "edge_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# convert to Torch tensors and create masks\n",
    "\n",
    "features = torch.FloatTensor(features)\n",
    "labels = torch.LongTensor(labels)\n",
    "\n",
    "# use first len(y)=140 elements for training\n",
    "train_index = torch.arange(y.shape[0], dtype=torch.long)\n",
    "train_mask = torch.zeros((labels.shape[0], ), dtype=torch.bool)\n",
    "train_mask[train_index] = 1\n",
    "\n",
    "# use next 500 elements for validation\n",
    "val_index = torch.arange(y.shape[0], y.shape[0] + 500, dtype=torch.long)\n",
    "val_mask = torch.zeros((labels.shape[0], ), dtype=torch.bool)\n",
    "val_mask[val_index] = 1\n",
    "\n",
    "# use next 1000 elements defined in test_index for testing\n",
    "test_index = torch.LongTensor(test_index)\n",
    "test_mask = torch.zeros((labels.shape[0], ), dtype=torch.bool)\n",
    "test_mask[test_index] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Data(edge_index=[2, 10556], test_mask=[2708], train_mask=[2708], val_mask=[2708], x=[2708, 1433], y=[2708])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create PyTorch Geometric dataset\n",
    "\n",
    "from torch_geometric.data import Data\n",
    "\n",
    "data = Data(edge_index=edge_index,\n",
    "            x=features,\n",
    "            y=labels,\n",
    "            train_mask=train_mask,\n",
    "            val_mask=val_mask,\n",
    "            test_mask=test_mask)\n",
    "\n",
    "torch.save(data, 'cora_dataset.pt')\n",
    "\n",
    "data\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}